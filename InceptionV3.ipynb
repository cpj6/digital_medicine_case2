{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"InceptionV3.ipynb","provenance":[{"file_id":"155S_bb3viIoL0wAwkIyr1r8XQu4ARwA9","timestamp":1604715169240}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1026cb834f0441a5883347ebfa88e252":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_118b496d00ab4a88bdbafff4d7208338","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ee65fa0850474af590dc3499d796395b","IPY_MODEL_2eff5fa4a3ab4002a6e633382abcf5d2"]}},"118b496d00ab4a88bdbafff4d7208338":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ee65fa0850474af590dc3499d796395b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_330d4d04db334e66884da31f027b4229","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":108857766,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":108857766,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4d392c82f3384dd194a89c4ed07af1f4"}},"2eff5fa4a3ab4002a6e633382abcf5d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_85b4e45cf47b4b9e984afb6a9c1a9fd3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 104M/104M [00:26&lt;00:00, 4.07MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d1ede508d27e4d04a814698138d5f5f3"}},"330d4d04db334e66884da31f027b4229":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4d392c82f3384dd194a89c4ed07af1f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"85b4e45cf47b4b9e984afb6a9c1a9fd3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d1ede508d27e4d04a814698138d5f5f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"rA1mm_GDe5eh"},"source":["  # <reference>\n","\n","# main\n","# https://github.com/wusaifei/HWCC_image_classification/blob/master/src/train.py\n","\n","# data_transforms2\n","# https://github.com/Anjum48/rsna-ich"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q4GS019De53E","executionInfo":{"status":"ok","timestamp":1605327865757,"user_tz":-480,"elapsed":17693,"user":{"displayName":"張哲魁","photoUrl":"","userId":"09477290535038822979"}},"outputId":"2412e7c9-5e6b-4465-c7a7-d56ac883a539","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dLMU5oi7fHMq","executionInfo":{"status":"ok","timestamp":1605327877104,"user_tz":-480,"elapsed":29032,"user":{"displayName":"張哲魁","photoUrl":"","userId":"09477290535038822979"}},"outputId":"2da3665b-afdc-476f-ed88-f726e18ff1d2","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install efficientnet_pytorch\n","!pip install pydicom\n","\n","# -*- coding: utf-8 -*-\n","from torchvision import datasets, transforms, models\n","import torch\n","from torch import nn\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","from torch.utils.data.dataloader import default_collate\n","from efficientnet_pytorch import EfficientNet\n","import numpy as np\n","import pydicom\n","import matplotlib.pyplot as plt\n","import glob\n","import os\n","from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n","from PIL import Image"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting efficientnet_pytorch\n","  Downloading https://files.pythonhosted.org/packages/4e/83/f9c5f44060f996279e474185ebcbd8dbd91179593bffb9abe3afa55d085b/efficientnet_pytorch-0.7.0.tar.gz\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.7.0+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.18.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.7)\n","Building wheels for collected packages: efficientnet-pytorch\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.0-cp36-none-any.whl size=16031 sha256=78d917e0331e4810cde2aa2100d87276be7536dbfb723b4baa48604f778f7917\n","  Stored in directory: /root/.cache/pip/wheels/e9/c6/e1/7a808b26406239712cfce4b5ceeb67d9513ae32aa4b31445c6\n","Successfully built efficientnet-pytorch\n","Installing collected packages: efficientnet-pytorch\n","Successfully installed efficientnet-pytorch-0.7.0\n","Collecting pydicom\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/7b/6ed88f82dd33a32cdb43432dab7f84fcd40c49d63251442b3cfe0be983d4/pydicom-2.1.1-py3-none-any.whl (1.9MB)\n","\u001b[K     |████████████████████████████████| 1.9MB 8.5MB/s \n","\u001b[?25hInstalling collected packages: pydicom\n","Successfully installed pydicom-2.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bLzf3KR1fKaK"},"source":["data_dir = \"/content/drive/Shared drives/數位醫學/Case Presentation 2\"\n","\n","# hyper parameters\n","num_classes = 6\n","batch_size = 16\n","#EPOCH = 150\n","EPOCH = 40\n","#input_size = 256\n","input_size = 299\n","#input_size = 512\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LnEcCupkfN5r"},"source":["# windowing\n","def correct_dcm(dcm):\n","    x = dcm.pixel_array + 1000\n","    px_mode = 4096\n","    x[x>=px_mode] = x[x>=px_mode] - px_mode\n","    dcm.PixelData = x.tobytes()\n","    dcm.RescaleIntercept = -1000\n","\n","def window_image(dcm, window_center, window_width):\n","    \n","    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n","      correct_dcm(dcm)\n","    \n","    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n","    img_min = window_center - window_width // 2\n","    img_max = window_center + window_width // 2\n","    img = np.clip(img, img_min, img_max)\n","\n","    return img\n","\n","def bsb_window(dcm):\n","    brain_img = window_image(dcm, 40, 80)\n","    subdural_img = window_image(dcm, 80, 200)\n","    #soft_img = window_image(dcm, 40, 380)\n","    bone_img = window_image(dcm , 600 , 2800)\n","\n","    brain_img = (brain_img - 0) / 80\n","    subdural_img = (subdural_img - (-20)) / 200\n","    #soft_img = (soft_img - (-150)) / 380\n","    bone_img = (bone_img - (-800)) / 2800\n","\n","    #bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n","    bsb_img = np.array([brain_img, subdural_img, bone_img]).transpose(1,2,0)\n","\n","    return bsb_img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MZAoWyLqfPVq"},"source":["class myDataset(Dataset):\n","  def __init__(self, folder, classes, transform=None):\n","\n","    self.dictonary = {'epidural':0, 'healthy':1, 'intraparenchymal':2, 'intraventricular':3, 'subarachnoid':4, 'subdural':5}\n","    self.path = []\n","    self.label = []\n","    self.transform = transform\n","\n","    # train data\n","    # self.path存dcm的path\n","    # self.label存dcm的label\n","    if len(classes) > 0:\n","      for i in classes:\n","        for j in glob.glob(folder + '/' + i + '/*'):\n","          self.path.append(j)\n","          self.label.append(self.dictonary[i])\n","    # test data的部分之後寫\n","    # 可能再寫一個test data的myDataset\n","    #else:\n","      #pass\n","\n","  def __len__(self):\n","    return len(self.path)\n"," \n","  def __getitem__(self, idx):\n","    #img = torch.from_numpy(bsb_window(pydicom.dcmread(self.path[idx])).transpose(2, 0, 1)).type(torch.FloatTensor)\n","\n","    img = bsb_window(pydicom.dcmread(self.path[idx])) # C*H*W => H*W*C\n","    # 以下動作使得圖片得以轉為PIL，才能使用部分的transform\n","    img *= 255 # [0., 1.] => [0., 255.]\n","    img = img.astype(np.uint8) # [0., 255.] => [0, 255]\n","    img = Image.fromarray(img) # numpy => PIL\n","\n","    #print(np.max(np.array(img)), np.min(np.array(img)), np.mean(np.array(img)), np.median(np.array(img)))\n","    if self.transform:\n","      img = self.transform(img)\n","    #print(torch.max(img), torch.min(img), torch.mean(img), torch.median(img))\n","    return img, self.label[idx]\n","\n","classes = ['epidural', 'healthy', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n","\n","# data augmentation\n","data_transforms2 = transforms.Compose([\n","  #transforms.ToPILImage(),                                       \n","  transforms.RandomHorizontalFlip(p=0.5),\n","  transforms.RandomRotation(degrees=15),\n","  transforms.RandomResizedCrop(input_size, scale=(0.85, 1.0), ratio=(0.8, 1.2)),\n","  transforms.ToTensor(), # PIL => FloatTensor，[0, 255] => [0., 1.]\n","])\n","\n","TrainDataset = myDataset(folder='/content/drive/Shared drives/數位醫學/Case Presentation 2/TrainingData_tmp', classes=classes, transform=data_transforms2)\n","\n","\n","validation_split = .2\n","shuffle_dataset = True\n","random_seed = 2020\n","\n","dataset_size = len(TrainDataset)\n","indices = list(range(dataset_size))\n","split = int(np.floor(validation_split * dataset_size))\n","if shuffle_dataset:\n","  np.random.seed(random_seed)\n","  np.random.shuffle(indices)\n","train_indices, val_indices = indices[split:], indices[:split]\n","\n","train_sampler = SubsetRandomSampler(train_indices)\n","valid_sampler = SubsetRandomSampler(val_indices)\n","\n","train_loader = torch.utils.data.DataLoader(TrainDataset, batch_size=batch_size, sampler=train_sampler, num_workers=4)\n","validation_loader = torch.utils.data.DataLoader(TrainDataset, batch_size=batch_size, sampler=valid_sampler, num_workers=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8htMo8B_fRvy","executionInfo":{"status":"ok","timestamp":1605327930996,"user_tz":-480,"elapsed":82908,"user":{"displayName":"張哲魁","photoUrl":"","userId":"09477290535038822979"}},"outputId":"cf39a084-5603-4e6e-8836-c1660158cfe9","colab":{"base_uri":"https://localhost:8080/","height":82,"referenced_widgets":["1026cb834f0441a5883347ebfa88e252","118b496d00ab4a88bdbafff4d7208338","ee65fa0850474af590dc3499d796395b","2eff5fa4a3ab4002a6e633382abcf5d2","330d4d04db334e66884da31f027b4229","4d392c82f3384dd194a89c4ed07af1f4","85b4e45cf47b4b9e984afb6a9c1a9fd3","d1ede508d27e4d04a814698138d5f5f3"]}},"source":["# load pretrained weight(trained from ImageNet)\n","net = models.inception_v3(pretrained=True)\n","#set_parameter_requires_grad(net, feature_extract)\n","# Handle the auxilary net\n","num_ftrs = net.AuxLogits.fc.in_features\n","net.AuxLogits.fc = nn.Linear(num_ftrs, 6)\n","# Handle the primary net\n","num_ftrs = net.fc.in_features\n","net.fc = nn.Linear(num_ftrs,6)\n","#net.dropout = nn.Dropout(0.95)\n","net = net.to(device)\n","input_size = 299"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1026cb834f0441a5883347ebfa88e252","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=108857766.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QR9PPulzsSma"},"source":["ii = 0\n","LR = 0.0005  # learning rate\n","best_acc = 0  # initialize best test accuracy\n","\n","# loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# optimizer\n","#optimizer = optim.Adam(net.parameters(), lr=LR)\n","#optimizer = optim.Adam(net.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-9)\n","optimizer = optim.Adam(net.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-9, weight_decay=1e-5)\n","#optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9)\n","\n","scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=3, verbose=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Av5XDj64lpVO","executionInfo":{"status":"error","timestamp":1605328654075,"user_tz":-480,"elapsed":805967,"user":{"displayName":"張哲魁","photoUrl":"","userId":"09477290535038822979"}},"outputId":"47e0c06f-163a-48e7-dce0-7260c01a9f6d","colab":{"base_uri":"https://localhost:8080/","height":859}},"source":["\n","import datetime\n","## modified region ## \n","iter_train = []\n","loss_train = []\n","loss_val = []\n","acc_train = []\n","valid_acc_train = [] \n","epoch_train = []\n","## modified region ## \n","##-------AUC-------##\n","y_label=[] \n","y_score=[]\n","##-------AUC-------##\n","best_val_acc = 0.0\n","## modfied region (11/13 09:41) ##\n","BEST_LOSS_VAL = 10.0\n","## modfied region (11/13 09:41) ##\n","for epoch in range(EPOCH):\n","  net.train()\n","  sum_loss = 0.0\n","  correct = 0.0\n","  total = 0.0\n","  ## modified region ## \n","  LOSS_TRAIN = 0.0\n","  starttime = datetime.datetime.now()\n","  # training\n","  for i, data in enumerate(train_loader, 0):\n","    length = len(train_loader)\n","    input, target = data\n","    #print(target, type(target))\n","    input, target = input.to(device), target.to(device)\n","    \n","    optimizer.zero_grad()\n","\n","    #output = net(input)\n","    output, aux_outputs = net(input)\n","    #loss = criterion(output, target)\n","    loss1 = criterion(output, target)\n","    loss2 = criterion(aux_outputs, target)\n","    loss = loss1 + 0.4*loss2\n","    loss.backward()\n","    optimizer.step()\n","\n","    sum_loss += loss.item()\n","    _, predicted = torch.max(output.data, 1)\n","    total += target.size(0)\n","    correct += predicted.eq(target.data).cpu().sum()\n","    if i % 80 == 0:\n","      print('[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% ' \n","        % (epoch + 1, (i + 1 + epoch * length), sum_loss / (i + 1),\n","        100. * float(correct) / float(total)))\n","    LOSS_TRAIN = sum_loss / (i + 1)\n","\n","  ## modified region ##    \n","  epoch_train.append(epoch)\n","  loss_train.append(LOSS_TRAIN)\n","  acc_train.append(100. * float(correct) / float(total))   \n","  ## modified region ## \n","\n","   \n","  # validation\n","  with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    val_sum_loss = 0\n","    LOSS_VAL = 0\n","    for i, data in enumerate(validation_loader, 0):\n","      net.eval()\n","      images, labels = data\n","      images, labels = images.to(device), labels.to(device)\n","\n","      ## modfied region (11/13 09:41) ##\n","      optimizer.zero_grad()\n","      ## modfied region (11/13 09:41) ##\n","\n","      #outputs = net(images)\n","      output = net(images)\n","      loss = criterion(output, labels)\n","      val_sum_loss += loss.item()\n","      # 取得分最高的那個類 (outputs.data的索引號)\n","      _, predicted = torch.max(output.data, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).cpu().sum()\n","      LOSS_VAL = val_sum_loss / (i + 1)\n","      #------------FOR AUC----------------\n","      for j in output:\n","        j.cpu().numpy()\n","        y_score.append(j)\n","      for p,l in zip(predicted, labels):\n","        y_label.append(l.cpu().numpy())\n","      #------------FOR AUC----------------\n","    print('validation loss:', LOSS_VAL)\n","    loss_val.append(LOSS_VAL)\n","    print('validation accuracy：%.3f%%' % (100. * float(correct) / float(total)))\n","    ## modified region ## \n","    valid_acc_train.append(100. * float(correct) / float(total))\n","    ## modified region ## \n","    acc = 100. * float(correct) / float(total)\n","    if acc > best_val_acc:\n","      print(\"save weight!! (acc)\")\n","      torch.save(net, '/content/drive/Shared drives/數位醫學/Case Presentation 2/Implementation/weights/model=inceptionv3_acc.pkl')\n","      best_val_acc = acc\n","    ## modfied region (11/13 09:41) ##\n","    if LOSS_VAL < BEST_LOSS_VAL:\n","      print(\"save weight!! (loss)\")\n","      torch.save(net, '/content/drive/Shared drives/數位醫學/Case Presentation 2/Implementation/weights/model=inceptionv3_loss.pkl')\n","      BEST_LOSS_VAL = LOSS_VAL\n","    ## modfied region (11/13 09:41) ##\n","    scheduler.step(acc)\n","  endtime = datetime.datetime.now()\n","  print((endtime - starttime).seconds,\" seconds\")\n","    \n","\n","\n","print(\"Training Finished, Total EPOCH=%d\" % EPOCH)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[epoch:1, iter:1] Loss: 2.658 | Acc: 12.500% \n","[epoch:1, iter:81] Loss: 2.404 | Acc: 29.244% \n","[epoch:1, iter:161] Loss: 2.283 | Acc: 33.385% \n","[epoch:1, iter:241] Loss: 2.195 | Acc: 36.255% \n","validation loss: 1.8713670818010966\n","validation accuracy：39.000%\n","save weight!! (acc)\n","save weight!! (loss)\n","569  seconds\n","[epoch:2, iter:301] Loss: 1.180 | Acc: 75.000% \n","[epoch:2, iter:381] Loss: 1.873 | Acc: 49.383% \n","[epoch:2, iter:461] Loss: 1.861 | Acc: 49.806% \n","[epoch:2, iter:541] Loss: 1.841 | Acc: 50.467% \n","validation loss: 1.3616236273447673\n","validation accuracy：49.333%\n","save weight!! (acc)\n","save weight!! (loss)\n","71  seconds\n","[epoch:3, iter:601] Loss: 1.642 | Acc: 43.750% \n","[epoch:3, iter:681] Loss: 1.658 | Acc: 56.636% \n","[epoch:3, iter:761] Loss: 1.647 | Acc: 56.056% \n","[epoch:3, iter:841] Loss: 1.644 | Acc: 56.639% \n","validation loss: 1.139559456507365\n","validation accuracy：55.833%\n","save weight!! (acc)\n","save weight!! (loss)\n","75  seconds\n","[epoch:4, iter:901] Loss: 1.192 | Acc: 75.000% \n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-a836cda06b58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0msum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"Igy1Eu4dWS5n"},"source":["\n","\n","plt.figure(figsize=(12.8, 9.6)) \n","plt.xlabel(\"Epoch\")\n","plt.xlim(0 , len(epoch_train)-1)\n","plt.ylim(0 , 100)\n","plt.ylabel(\"Accuracy\") \n","plt.title(\"InceptionV3 statistics\")\n","plt.plot(epoch_train , acc_train , label = \"Acc\")\n","#plt.plot(epoch_train , loss_train , label = \"Loss\")\n","plt.plot(epoch_train , valid_acc_train , label = \"Valid_acc\")\n","plt.legend(loc=\"upper left\")\n","plt.grid()\n","plt.show()  \n","plt.savefig(\"InceptionV3.jpg\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F5zdByNJTnd-"},"source":["plt.figure(figsize=(12.8, 9.6)) \n","plt.plot(epoch_train , loss_train , label = \"Train_Loss\")\n","plt.plot(epoch_train , loss_val, label='Val_Loss')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EuTHHDPZcEHZ"},"source":["## modfied region (11/13 09:41) ##\n","y_score_tmp = y_score\n","y_label_tmp = y_label\n","## modfied region (11/13 09:41) ##\n","len(y_label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"akMII3hI-uKZ"},"source":["a = []\n","for i in range(len(y_score)):\n","  a.append((y_score[i][y_label[i+42000]]))\n","plt.hist(np.array(a), bins=20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U9ldip6olKIT"},"source":["import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from itertools import cycle\n","from sklearn.metrics import roc_curve, auc\n","from scipy import interp\n","\n","\n","y_score = np.array([F.softmax(x.to(\"cpu\"),dim=0).numpy().tolist() for x in y_score])\n","y_label = [x.item() for x in y_label]\n","one_hot_label = []\n","\n","for i in range(len(y_label)):\n","  six_zero = [0, 0, 0, 0, 0, 0]\n","  six_zero[y_label[i]] = 1\n","  one_hot_label.append(six_zero)\n","one_hot_label = np.array(one_hot_label)\n","\n","\n","n_classes = 6\n","fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","for i in range(n_classes):\n","  fpr[i], tpr[i], _ = roc_curve(one_hot_label[:, i], y_score[:, i])\n","  roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","# macro（方法一）\n","# First aggregate all false positive rates\n","all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n","# Then interpolate all ROC curves at this points\n","mean_tpr = np.zeros_like(all_fpr)\n","for i in range(n_classes):\n","  mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n","# Finally average it and compute AUC\n","mean_tpr /= n_classes\n","fpr[\"macro\"] = all_fpr\n","tpr[\"macro\"] = mean_tpr\n","roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n"," \n","# Plot all ROC curves\n","lw=2\n","plt.figure()\n"," \n","plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n","         label='macro-average ROC curve (area = {0:0.2f})'\n","               ''.format(roc_auc[\"macro\"]),\n","         color='navy', linestyle=':', linewidth=4)\n"," \n","colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'lime', 'violet', 'hotpink'])\n","\n","classes_name = ['epidural', 'healthy', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n","\n","for i, color in zip(range(n_classes), colors):\n","    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n","             label='ROC curve of {0} (area = {1:0.2f})'\n","             ''.format(classes_name[i], roc_auc[i]))\n","\n","plt.rcParams[\"figure.figsize\"] = (50,50)\n","plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n","plt.xlim([-0.05, 1.05])\n","plt.ylim([-0.05, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('multi-calss ROC')\n","plt.legend(loc=\"lower right\")\n","plt.grid()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eKDnJKHqQ1fH"},"source":[""],"execution_count":null,"outputs":[]}]}